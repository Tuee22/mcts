services:
  # CPU service - no GPU requirements
  mcts:
    profiles: ["cpu"]
    build:
      context: ..
      dockerfile: docker/Dockerfile
      args:
        VARIANT: cpu
        BASE_IMAGE: ${CPU_BASE_IMAGE:-ubuntu:22.04}
    image: mcts:cpu
    ports:
      - target: 8000
        published: ${PORT:-8000}
        protocol: tcp
    restart: unless-stopped
    environment:
      - PYTHONPATH=/app/backend/python
      - NPM_CONFIG_YES=true
    volumes:
      # Mount entire project for live development
      - type: bind
        source: ..
        target: /app
        read_only: false
    command: ${APP_COMMAND:-bash -c "poetry --version && python --version && echo 'MCTS CPU container ready' && tail -f /dev/null"}

  # GPU service - with NVIDIA runtime
  mcts-gpu:
    profiles: ["gpu"]
    build:
      context: ..
      dockerfile: docker/Dockerfile
      args:
        VARIANT: cuda
        BASE_IMAGE: ${CUDA_BASE_IMAGE:-nvidia/cuda:12.6.2-devel-ubuntu22.04}
    image: mcts:cuda
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GPU_COUNT:-all}
              capabilities: [gpu]
    ports:
      - target: 8000
        published: ${PORT:-8000}
        protocol: tcp
    restart: unless-stopped
    environment:
      - PYTHONPATH=/app/backend/python
      - NPM_CONFIG_YES=true
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
    volumes:
      # Mount entire project for live development
      - type: bind
        source: ..
        target: /app
        read_only: false
    command: ${APP_COMMAND:-bash -c "nvidia-smi || true; poetry --version && python --version && echo 'MCTS CUDA container ready' && tail -f /dev/null"}